{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harry Chong\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "import logging\n",
    "import base64\n",
    "import io\n",
    "import IPython\n",
    "import imageio\n",
    "from tensorflow import keras\n",
    "\n",
    "from gym.wrappers import TimeLimit\n",
    "import tf_agents\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ROOT_DIR = \".\"\n",
    "env = suite_gym.load(\"Assault-v4\")\n",
    "env.seed(42)\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Wrappers\n",
    "# Wrapper to repeat same action n steps and accumlates the rewards (speed up training)\n",
    "action_repeat_env = ActionRepeat(env, times=4)\n",
    "\n",
    "# Wrapper to interrupt env if it runs longer than a max number of steps\n",
    "time_limit_env = suite_gym.load(\"Assault-v4\", \n",
    "    gym_env_wrappers = [lambda env: TimeLimit(env, max_episode_steps = 10000)], \n",
    "    env_wrappers=[lambda env: ActionRepeat(env, times = 4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Pre-Processing\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"AssaultNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps = max_episode_steps,\n",
    "    gym_env_wrappers = [AtariPreprocessing, FrameStack4]\n",
    ")\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Assault Env.\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "time_step = env.step(np.array(1)) # FIRE\n",
    "for _ in range(4):\n",
    "    time_step = env.step(np.array(3)) # LEFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep Q-Network with TF-Agents\n",
    "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(64, (8, 8), 4), (64, (3, 3), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[1024]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    activation_fn=tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See TF-agents issue #113\n",
    "#optimizer = keras.optimizers.RMSprop(lr = 2.5e-4, rho = 0.95, momentum = 0.0, epsilon = 0.00001, centered = True)\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate = 1e-3, decay = 0.95, \n",
    "    momentum = 0.0, epsilon = 0.0001, centered = True)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = 1.0, # initial ?\n",
    "    decay_steps = 25000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate = 0.01\n",
    ") # final ?\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=  q_net,\n",
    "    optimizer = optimizer,\n",
    "    target_update_period = 2000, # <=> 32,000 ALE frames\n",
    "    td_errors_loss_fn = keras.losses.Huber(reduction=\"none\"),\n",
    "    gamma = 0.95, # discount factor\n",
    "    train_step_counter = train_step,\n",
    "    epsilon_greedy = lambda: epsilon_fn(train_step),\n",
    "    reward_scale_factor = 1.5\n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Replay Buffer and Observer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = agent.collect_data_spec,\n",
    "    batch_size = tf_env.batch_size,\n",
    "    max_length = 1000000)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom observer that counts and displays the number of times it is called\n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training metrics\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "train_metrics[0].result()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collect driver\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the initial experiences, before training\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers = [replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps = 20000) # <=> 80,000 ALE frames\n",
    "\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "trajectories, buffer_info = replay_buffer.get_next(\n",
    "    sample_batch_size=2, num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories._fields\n",
    "# trajectories.observation.shape\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape\n",
    "trajectories.step_type.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish checkpoint for later training\n",
    "checkpoint_dir = os.path.join(\"lastModelCheckpoint\")\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=agent)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './lastModelCheckpoint', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the main functions to TF Functions for better performance\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "        \n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "        if iteration % 5000 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Train the agent for 75,000 steps. Repeat as much as you want, the agent will keep improving.\n",
    "# Load from latest checkpoint if possible.\n",
    "train_agent(n_iterations=75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save frames for updated animated gif\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "def reset_and_fire_on_life_lost(trajectory):\n",
    "    global prev_lives\n",
    "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "    if prev_lives != lives:\n",
    "        tf_env.reset()\n",
    "        tf_env.pyenv.envs[0].step(np.array(1))\n",
    "        prev_lives = lives\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create updated animated gif of agent in action\n",
    "image_path = os.path.join(PROJECT_ROOT_DIR, \"myAgentPlays.gif\")\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images[0].save(image_path, format = 'GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all = True,\n",
    "                     duration = 30,\n",
    "                     loop = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"./myAgentPlays.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save policy and model\n",
    "policy_dir = os.path.join(PROJECT_ROOT_DIR, \"savedPolicy\")\n",
    "tf_policy_saver = PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

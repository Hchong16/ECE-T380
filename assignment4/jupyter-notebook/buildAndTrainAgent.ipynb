{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harry Chong\n",
    "import gym\n",
    "import numpy as np\n",
    "import os\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.animation as animation\n",
    "import PIL\n",
    "import logging\n",
    "import base64\n",
    "import io\n",
    "import IPython\n",
    "import imageio\n",
    "from tensorflow import keras\n",
    "\n",
    "from gym.wrappers import TimeLimit\n",
    "import tf_agents\n",
    "from tf_agents.environments import suite_gym\n",
    "from tf_agents.environments.wrappers import ActionRepeat\n",
    "from tf_agents.environments import suite_atari\n",
    "from tf_agents.environments.atari_preprocessing import AtariPreprocessing\n",
    "from tf_agents.environments.atari_wrappers import FrameStack4\n",
    "from tf_agents.environments.tf_py_environment import TFPyEnvironment\n",
    "\n",
    "from tf_agents.networks.q_network import QNetwork\n",
    "from tf_agents.agents.dqn.dqn_agent import DqnAgent\n",
    "from tf_agents.replay_buffers import tf_uniform_replay_buffer\n",
    "from tf_agents.metrics import tf_metrics\n",
    "\n",
    "from tf_agents.drivers.dynamic_step_driver import DynamicStepDriver\n",
    "from tf_agents.policies.random_tf_policy import RandomTFPolicy\n",
    "from tf_agents.policies.policy_saver import PolicySaver\n",
    "from tf_agents.eval.metric_utils import log_metrics\n",
    "from tf_agents.trajectories.trajectory import to_transition\n",
    "from tf_agents.utils.common import function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make this notebook's output stable across runs\n",
    "np.random.seed(42)\n",
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Where to save the figures\n",
    "PROJECT_ROOT_DIR = \".\"\n",
    "IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, \"images\")\n",
    "os.makedirs(IMAGES_PATH, exist_ok=True)\n",
    "\n",
    "def save_fig(fig_id, tight_layout=True, fig_extension=\"png\", resolution=300):\n",
    "    path = os.path.join(IMAGES_PATH, fig_id + \".\" + fig_extension)\n",
    "    print(\"Saving figure\", fig_id)\n",
    "    if tight_layout:\n",
    "        plt.tight_layout()\n",
    "    plt.savefig(path, format=fig_extension, dpi=resolution)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gym.envs.registry.all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = suite_gym.load(\"Assault-v4\")\n",
    "env.seed(42)\n",
    "env.reset();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Retrieve image of environment \n",
    "def plot_environment(env):\n",
    "    img = env.render(mode=\"rgb_array\")\n",
    "    plt.figure(figsize=(6, 8))\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    return img\n",
    "\n",
    "plot_environment(env)\n",
    "save_fig(\"assault_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detail regarding observations, action, and one step of play \n",
    "env.observation_spec()\n",
    "env.action_spec()\n",
    "env.time_step_spec()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Environment Wrappers\n",
    "# Wrapper to repeat same action n steps and accumlates the rewards (speed up training)\n",
    "action_repeat_env = ActionRepeat(env, times=4)\n",
    "\n",
    "# Wrapper to interrupt env if it runs longer than a max number of steps\n",
    "time_limit_env = suite_gym.load(\"Assault-v4\", \n",
    "    gym_env_wrappers = [lambda env: TimeLimit(env, max_episode_steps = 10000)], \n",
    "    env_wrappers=[lambda env: ActionRepeat(env, times = 4)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Atari Pre-Processing\n",
    "max_episode_steps = 27000 # <=> 108k ALE frames since 1 step = 4 frames\n",
    "environment_name = \"AssaultNoFrameskip-v4\"\n",
    "\n",
    "env = suite_atari.load(\n",
    "    environment_name,\n",
    "    max_episode_steps = max_episode_steps,\n",
    "    gym_env_wrappers = [AtariPreprocessing, FrameStack4]\n",
    ")\n",
    "\n",
    "tf_env = TFPyEnvironment(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo Assault Env.\n",
    "env.seed(42)\n",
    "env.reset()\n",
    "\n",
    "time_step = env.step(np.array(1)) # FIRE\n",
    "for _ in range(4):\n",
    "    time_step = env.step(np.array(3)) # LEFT\n",
    "    \n",
    "def plot_observation(obs):\n",
    "    # Since there are only 3 color channels, you cannot display 4 frames\n",
    "    # with one primary color per frame. So this code computes the delta between\n",
    "    # the current frame and the mean of the other frames, and it adds this delta\n",
    "    # to the red and blue channels to get a pink color for the current frame.\n",
    "    obs = obs.astype(np.float32)\n",
    "    img = obs[..., :3]\n",
    "    current_frame_delta = np.maximum(obs[..., 3] - obs[..., :3].mean(axis=-1), 0.)\n",
    "    img[..., 0] += current_frame_delta\n",
    "    img[..., 2] += current_frame_delta\n",
    "    img = np.clip(img / 150, 0, 1)\n",
    "    plt.imshow(img)\n",
    "    plt.axis(\"off\")\n",
    "    \n",
    "plt.figure(figsize = (6, 6))\n",
    "plot_observation(time_step.observation)\n",
    "save_fig(\"preprocessed_assault_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Deep Q-Network with TF-Agents\n",
    "preprocessing_layer = keras.layers.Lambda(lambda obs: tf.cast(obs, np.float32) / 255.)\n",
    "conv_layer_params=[(64, (8, 8), 4), (64, (3, 3), 2), (64, (3, 3), 1)]\n",
    "fc_layer_params=[1024]\n",
    "\n",
    "q_net = QNetwork(\n",
    "    tf_env.observation_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    preprocessing_layers=preprocessing_layer,\n",
    "    conv_layer_params=conv_layer_params,\n",
    "    fc_layer_params=fc_layer_params,\n",
    "    activation_fn=tf.keras.activations.relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# See TF-agents issue #113\n",
    "train_step = tf.Variable(0)\n",
    "update_period = 4 # run a training step every 4 collect steps\n",
    "\n",
    "optimizer = tf.compat.v1.train.RMSPropOptimizer(learning_rate = 0.0001, decay = 0.95, \n",
    "    momentum = 0.0, epsilon = 0.01, centered = True)\n",
    "\n",
    "epsilon_fn = keras.optimizers.schedules.PolynomialDecay(\n",
    "    initial_learning_rate = 1.0, # initial ?\n",
    "    decay_steps = 25000 // update_period, # <=> 1,000,000 ALE frames\n",
    "    end_learning_rate = 0.01\n",
    ") # final ?\n",
    "\n",
    "agent = DqnAgent(tf_env.time_step_spec(),\n",
    "    tf_env.action_spec(),\n",
    "    q_network=  q_net,\n",
    "    optimizer = optimizer,\n",
    "    target_update_period = 2000, # <=> 32,000 ALE frames\n",
    "    td_errors_loss_fn = keras.losses.Huber(reduction=\"none\"),\n",
    "    gamma = 0.95, # discount factor\n",
    "    train_step_counter = train_step,\n",
    "    epsilon_greedy = lambda: epsilon_fn(train_step),\n",
    "    reward_scale_factor = 1.5\n",
    ")\n",
    "\n",
    "agent.initialize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Replay Buffer and Observer\n",
    "replay_buffer = tf_uniform_replay_buffer.TFUniformReplayBuffer(\n",
    "    data_spec = agent.collect_data_spec,\n",
    "    batch_size = tf_env.batch_size,\n",
    "    max_length = 1000000)\n",
    "\n",
    "replay_buffer_observer = replay_buffer.add_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom observer that counts and displays the number of times it is called\n",
    "class ShowProgress:\n",
    "    def __init__(self, total):\n",
    "        self.counter = 0\n",
    "        self.total = total\n",
    "    def __call__(self, trajectory):\n",
    "        if not trajectory.is_boundary():\n",
    "            self.counter += 1\n",
    "        if self.counter % 100 == 0:\n",
    "            print(\"\\r{}/{}\".format(self.counter, self.total), end = \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create training metrics\n",
    "train_metrics = [\n",
    "    tf_metrics.NumberOfEpisodes(),\n",
    "    tf_metrics.EnvironmentSteps(),\n",
    "    tf_metrics.AverageReturnMetric(),\n",
    "    tf_metrics.AverageEpisodeLengthMetric(),\n",
    "]\n",
    "\n",
    "train_metrics[0].result()\n",
    "\n",
    "logging.getLogger().setLevel(logging.INFO)\n",
    "log_metrics(train_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create collect driver\n",
    "collect_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.collect_policy,\n",
    "    observers=[replay_buffer_observer] + train_metrics,\n",
    "    num_steps=update_period) # collect 4 steps for each training iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Collect the initial experiences, before training\n",
    "initial_collect_policy = RandomTFPolicy(tf_env.time_step_spec(),\n",
    "                                        tf_env.action_spec())\n",
    "init_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    initial_collect_policy,\n",
    "    observers = [replay_buffer.add_batch, ShowProgress(20000)],\n",
    "    num_steps = 20000) # <=> 80,000 ALE frames\n",
    "\n",
    "final_time_step, final_policy_state = init_driver.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(888) # chosen to show an example of trajectory at the end of an episode\n",
    "\n",
    "trajectories, buffer_info = replay_buffer.get_next(\n",
    "    sample_batch_size=2, num_steps=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trajectories._fields\n",
    "# trajectories.observation.shape\n",
    "time_steps, action_steps, next_time_steps = to_transition(trajectories)\n",
    "time_steps.observation.shape\n",
    "trajectories.step_type.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sub episode plot\n",
    "plt.figure(figsize=(10, 6.8))\n",
    "for row in range(2):\n",
    "    for col in range(3):\n",
    "        plt.subplot(2, 3, row * 3 + col + 1)\n",
    "        plot_observation(trajectories.observation[row, col].numpy())\n",
    "plt.subplots_adjust(left = 0, right = 1, bottom = 0, top = 1, hspace = 0, wspace = 0.02)\n",
    "save_fig(\"sub_episodes_plot\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset\n",
    "dataset = replay_buffer.as_dataset(\n",
    "    sample_batch_size=64,\n",
    "    num_steps=2,\n",
    "    num_parallel_calls=3).prefetch(3)\n",
    "\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establish checkpoint for later training\n",
    "checkpoint_dir = os.path.join(\"lastModelCheckpoint\")\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, 'ckpt')\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, model=agent)\n",
    "manager = tf.train.CheckpointManager(checkpoint, './lastModelCheckpoint', max_to_keep=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the main functions to TF Functions for better performance\n",
    "collect_driver.run = function(collect_driver.run)\n",
    "agent.train = function(agent.train)\n",
    "\n",
    "def train_agent(n_iterations):\n",
    "    checkpoint.restore(manager.latest_checkpoint)\n",
    "    if manager.latest_checkpoint:\n",
    "        print(\"Restored from {}\".format(manager.latest_checkpoint))\n",
    "    else:\n",
    "        print(\"Initializing from scratch.\")\n",
    "        \n",
    "    time_step = None\n",
    "    policy_state = agent.collect_policy.get_initial_state(tf_env.batch_size)\n",
    "    iterator = iter(dataset)\n",
    "    for iteration in range(n_iterations):\n",
    "        time_step, policy_state = collect_driver.run(time_step, policy_state)\n",
    "        trajectories, buffer_info = next(iterator)\n",
    "        train_loss = agent.train(trajectories)\n",
    "        print(\"\\r{} loss:{:.5f}\".format(\n",
    "            iteration, train_loss.loss.numpy()), end=\"\")\n",
    "        if iteration % 1000 == 0:\n",
    "            log_metrics(train_metrics)\n",
    "        if iteration % 5000 == 0:\n",
    "            checkpoint.save(file_prefix = checkpoint_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the agent for 75,000 steps. Repeat as much as you want, the agent will keep improving.\n",
    "train_agent(n_iterations=75000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save frames for animated gif\n",
    "def update_scene(num, frames, patch):\n",
    "    patch.set_data(frames[num])\n",
    "    return patch\n",
    "\n",
    "def plot_animation(frames, repeat=False, interval=40):\n",
    "    fig = plt.figure()\n",
    "    patch = plt.imshow(frames[0])\n",
    "    plt.axis('off')\n",
    "    anim = animation.FuncAnimation(\n",
    "        fig, update_scene, fargs=(frames, patch),\n",
    "        frames=len(frames), repeat=repeat, interval=interval)\n",
    "    plt.close()\n",
    "    return anim\n",
    "\n",
    "frames = []\n",
    "def save_frames(trajectory):\n",
    "    global frames\n",
    "    frames.append(tf_env.pyenv.envs[0].render(mode=\"rgb_array\"))\n",
    "\n",
    "prev_lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "def reset_and_fire_on_life_lost(trajectory):\n",
    "    global prev_lives\n",
    "    lives = tf_env.pyenv.envs[0].ale.lives()\n",
    "    if prev_lives != lives:\n",
    "        tf_env.reset()\n",
    "        tf_env.pyenv.envs[0].step(np.array(1))\n",
    "        prev_lives = lives\n",
    "\n",
    "watch_driver = DynamicStepDriver(\n",
    "    tf_env,\n",
    "    agent.policy,\n",
    "    observers=[save_frames, reset_and_fire_on_life_lost, ShowProgress(1000)],\n",
    "    num_steps=1000)\n",
    "final_time_step, final_policy_state = watch_driver.run()\n",
    "\n",
    "plot_animation(frames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create animated gif of agent in action\n",
    "image_path = os.path.join(PROJECT_ROOT_DIR, \"myAgentPlays.gif\")\n",
    "frame_images = [PIL.Image.fromarray(frame) for frame in frames[:150]]\n",
    "frame_images[0].save(image_path, format = 'GIF',\n",
    "                     append_images=frame_images[1:],\n",
    "                     save_all = True,\n",
    "                     duration = 30,\n",
    "                     loop = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%html\n",
    "<img src=\"./myAgentPlays.gif\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save policy and and model\n",
    "policy_dir = os.path.join(PROJECT_ROOT_DIR, \"savedPolicy\")\n",
    "tf_policy_saver = PolicySaver(agent.policy)\n",
    "tf_policy_saver.save(policy_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_gif(gif_buffer):\n",
    "    \"\"\"Embeds a gif file in the notebook.\"\"\"\n",
    "    tag = '<img src=\"data:image/gif;base64,{0}\"/>'.format(base64.b64encode(gif_buffer).decode())\n",
    "    return IPython.display.HTML(tag)\n",
    "\n",
    "def run_episodes_and_create_video(policy, eval_tf_env, eval_py_env):\n",
    "    num_episodes = 3\n",
    "    frames = []\n",
    "    for _ in range(num_episodes):\n",
    "        time_step = eval_tf_env.reset()\n",
    "        frames.append(eval_py_env.render())\n",
    "        while not time_step.is_last():\n",
    "            action_step = policy.action(time_step)\n",
    "            time_step = eval_tf_env.step(action_step.action)\n",
    "            frames.append(eval_py_env.render())\n",
    "    gif_file = io.BytesIO()\n",
    "    imageio.mimsave(gif_file, frames, format='gif', fps=60)\n",
    "    IPython.display.display(embed_gif(gif_file.getvalue()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model and display video generation\n",
    "saved_policy = tf.compat.v2.saved_model.load(policy_dir)\n",
    "run_episodes_and_create_video(saved_policy, tf_env, env)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
